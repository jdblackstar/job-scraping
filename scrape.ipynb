{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "\n",
    "def create_logfile():\n",
    "    \"\"\"\n",
    "    :returns logging\n",
    "    \"\"\"\n",
    "    date = datetime.datetime.today().strftime('%d-%b-%y_%H:%M:%S')\n",
    "    logfile = f\"log/{date}.log\"\n",
    "    logging.basicConfig(filename=logfile, filemode='w', level=logging.INFO, \\\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s', \\\n",
    "                        datefmt='%d-%b-%y %H:%M:%S', force=True)\n",
    "    logging.info(f\"Log file {logfile} created.\")\n",
    "    return logging\n",
    "\n",
    "\n",
    "def create_file(file, logging):\n",
    "    \"\"\"\n",
    "    :param file\n",
    "    :param logging\n",
    "\n",
    "    :returns nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # if the daily file already exists, make sure to delete it to not clutter disk space\n",
    "    logging.info(\"Checking if current daily csv already exists...\")\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        logging.info(f\"     ...{file} deleted\")\n",
    "    else:\n",
    "        logging.info(f\"     ...{file} not found\")\n",
    "\n",
    "    # create file and add header\n",
    "    logging.info(\"Creating daily CSV...\")\n",
    "    header = ['date_time', 'search_keyword', 'search_count', 'job_id', 'job_title', \\\n",
    "              'company', 'location', 'remote', 'update_time', 'applicants', 'job_pay', \\\n",
    "              'job_time', 'job_position', 'company_size', 'company_industry', \\\n",
    "              'job_details']\n",
    "    with open(file, 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "        logging.info(f\"     ...{file} created\")\n",
    "\n",
    "\n",
    "def login(logging):\n",
    "    \"\"\"\n",
    "    :param logging\n",
    "\n",
    "    :returns wd - Chromium webdriver object from the selenium library\n",
    "    \"\"\"\n",
    "\n",
    "    url_login = 'https:///www.linkedin.com/'\n",
    "    load_dotenv()\n",
    "\n",
    "    # From .env file\n",
    "    LINKEDIN_USERNAME = os.getenv('LINKEDIN_USERNAME')\n",
    "    LINKEDIN_PASSWORD = os.getenv('LINKEDIN_PASSWORD')\n",
    "\n",
    "    # create headless instance of Chrome\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "\n",
    "    # Actually log in to LinkedIn\n",
    "    logging.info(f\"Looging into LinkedIn as {LINKEDIN_USERNAME}...\")\n",
    "    wd = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options)\n",
    "    wd.get(url_login)\n",
    "    wd.find_element_by_id('session_key').send_keys(LINKEDIN_USERNAME)\n",
    "    wd.find_element_by_id('session_password').send_keys(LINKEDIN_PASSWORD)\n",
    "    wd.find_element_by_xpath(\"//button[@class='sign-in-form__submit-button']\").click()\n",
    "\n",
    "    # not sure how often this happens, but sometimes there's a popup to confirm login info\n",
    "    try:\n",
    "        wd.find_element_by_xpath(\"//buton[@class='primary-action-new']\").click()\n",
    "    except:\n",
    "        pass\n",
    "    logging.info(\"     ...logged in\")\n",
    "    return wd\n",
    "\n",
    "\n",
    "def page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    - wd\n",
    "    - search_location\n",
    "    - search_keyword\n",
    "    - search_remote\n",
    "    - search_posted\n",
    "    - search_page\n",
    "    - search_count\n",
    "    - file\n",
    "    - logging\n",
    "    \"\"\"\n",
    "\n",
    "    page_wait = 30\n",
    "    click_wait = 5\n",
    "    async_wait = 5\n",
    "    retry_attempts = 3\n",
    "\n",
    "    url_search = f'https://www.linkedin.com/jobs/search/?f_TPR={search_posted}&f_WT={search_remote}&geoId=103644278&keywords={search_keyword}&location={search_location}&start={search_page}'\n",
    "\n",
    "    logging.info(\"Navigating to jobs page...\")\n",
    "    wd.get(url_search)\n",
    "    time.sleep(page_wait) # sneaky sneaky sleep\n",
    "    logging.info(\"     ...succeeded\")\n",
    "\n",
    "    # original parameter: \"//small[@class='display-flex t-12 t-black--light t-normal']\"\n",
    "    # second parameter: \"//small[@class='jobs-search-results-list__text display-flex t-12 t-black--light t-normal']\"\n",
    "    # third parameter: \"//small[@class='jobs-search-results-list__text display-flex t-12 t-black--light t-normal'] [@aria-live='polite']\"\n",
    "    # fourth parameter: \"/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/header/div[1]/small\"\n",
    "\n",
    "    # search_count = wd.find_element_by_xpath(\"//small[@class='jobs-search-results-list__text display-flex t-12 t-black--light t-normal']\").text\n",
    "    search_count = wd.find_element_by_css_selector(\"small.jobs-search-results-list__text\").text\n",
    "    search_count = int(search_count.split(' ')[0].replace(',', ''))\n",
    "    logging.info(f\"Loading page {round(search_page/25) + 1} of {round(search_count/25)} for {search_keyword}'s {search_count} results...\")\n",
    "\n",
    "    # collects job_ids for the current page\n",
    "    for attempt in range(retry_attempts):\n",
    "        try:\n",
    "            logging.info(\"Collecting job IDs for the current page...\")\n",
    "            search_results = wd.find_element_by_xpath(\"//ul[@class='jobs-search-results__list list-style-none']\").find_elements_by_tag_name(\"li\")\n",
    "            result_ids = [result.get_attribute('id') for result in search_results if result.get_attribute('id') != '']\n",
    "            break\n",
    "        except:\n",
    "            # EXCEPTION:\n",
    "            # wait a few attempts, if not throw an exception and then skip to next page\n",
    "            time.sleep(click_wait)\n",
    "        logging.info(\"     ...Successfully gathered all job IDs\")\n",
    "\n",
    "    # cycle through each id and append the job data to a new list called list_jobs\n",
    "    list_jobs = []\n",
    "    for res_id in result_ids:\n",
    "        try:\n",
    "            logging.info(\"Looking for the job id...\")\n",
    "            job = wd.find_element_by_id(res_id)\n",
    "            logging.info(\"     ...Found the job\")\n",
    "            logging.info(\"Getting attributes...\")\n",
    "            job_id = job.get_attribute(\"data-occludable-entity-urn\").split(':')[-1]\n",
    "            logging.info(f\"     ...Found. job_id = {job_id}\")\n",
    "            logging.info(\"Digging in deeper...\")\n",
    "            wd.find_element_by_xpath(f\"//div[@data-job-id='{job_id}']\").click()\n",
    "        except:\n",
    "            # EXCEPTION:\n",
    "            # exception probably caused by the job posting being deleted?\n",
    "            # either way, probably better to just except it here and skip forward\n",
    "            logging.error(\"     ...Couldn't find the job id div\")\n",
    "            continue\n",
    "\n",
    "        # FIND JOB TITLE\n",
    "        for attempt in range(retry_attempts):\n",
    "            try:\n",
    "                logging.info(\"Getting the Job title...\")\n",
    "                job_title = wd.find_element_by_xpath(\"//h2[@class='t-24 t-bold']\")\n",
    "                logging.info(\"     ...Found the job title\")\n",
    "                job_title = job_title.text\n",
    "                break\n",
    "            except:\n",
    "                # EXCEPTION:\n",
    "                # having some issues with the xpath thing up there ^\n",
    "                # making an exception here to just wait for the click delay\n",
    "                # then move to the next job\n",
    "                job_title = ''\n",
    "                time.sleep(click_wait)\n",
    "                logging.error(\"     ...Couldn't find the job title\")\n",
    "        \n",
    "        # GET COMPANY NAME, LOCATION AND REMOTE STATUS\n",
    "        for attempt in range(retry_attempts):\n",
    "            try:\n",
    "                logging.info(\"Looking for the company name, location and if this job is remote...\")\n",
    "                job_top_card = wd.find_element_by_xpath(\"//span[@class='jobs-unified-top-card__subtitle-primary-grouping mr2 t-black']\").find_elements_by_tag_name(\"span\")\n",
    "                logging.info(\"     ...found\")\n",
    "                company = job_top_card[0].text\n",
    "                location = job_top_card[1].text\n",
    "                if len(job_top_card) > 2:\n",
    "                    # the format of LinkedIn job cards is like \n",
    "                    # Company, Location, (Remote)\n",
    "                    # so we'll only grab the Remote tag if it's present\n",
    "                    remote = job_top_card[2].text\n",
    "                else:\n",
    "                    # Otherwise, set Remote to an empty string\n",
    "                    remote = ''\n",
    "                break\n",
    "            except:\n",
    "                logging.error(\"     ...wasn't able to find these attributes\")\n",
    "                company = ''\n",
    "                location = ''\n",
    "                remote = ''\n",
    "                time.sleep(click_wait)\n",
    "\n",
    "        # Get date posted (or reposted) and number of applicants\n",
    "        for attempt in range(retry_attempts):\n",
    "            try:\n",
    "                # TODO: since find_element_by_xpath is depreciated, let's replace with the right version\n",
    "                job_top_card2 = wd.find_element_by_xpath(\"//span[@class='jobs-unified-top-card__subtitle-secondary-grouping t-black--light']\").find_elements_by_tag_name(\"span\")\n",
    "                update_time = job_top_card2[0].text\n",
    "                applicants = job_top_card2[1].text.split(' ')[0]\n",
    "                break\n",
    "            except:\n",
    "                update_time = ''\n",
    "                applicants = ''\n",
    "                time.sleep(click_wait)\n",
    "                logging.error(\"Exception at 0004\")\n",
    "\n",
    "        job_time = ''\n",
    "        job_position = ''\n",
    "        job_pay = ''\n",
    "\n",
    "        for attempt in range(retry_attempts):\n",
    "            try:\n",
    "                # TODO: since find_element_by_xpath is depreciated, let's replace with the right version\n",
    "                # make sure HTML element is loaded\n",
    "                element = WebDriverWait(wd, 10).until(ec.presence_of_all_elements_located((By.XPATH, \"//div[@class='mt5 mb2']/div[1]\")))\n",
    "                # make sure text is loaded\n",
    "                try:\n",
    "                    job_info = element.text\n",
    "                    if job_info != '':\n",
    "                        # separate job info on time requirements and position\n",
    "                        job_info = job_info.split(\" · \")\n",
    "                        if len(job_info) == 1:\n",
    "                            job_pay = ''\n",
    "                            job_time = job_info[0]\n",
    "                            job_position = ''\n",
    "                        elif (len(job_info) >= 2) and (\"$\" in job_info[0]):\n",
    "                            job_pay = job_info[0]\n",
    "                            job_time = job_info[1]\n",
    "                            if len(job_info) >= 3:\n",
    "                                job_position = job_info[2]\n",
    "                            else:\n",
    "                                job_position = ''\n",
    "                        else:\n",
    "                            job_time = job_info[0]\n",
    "                            job_position = job_info[1]\n",
    "                            job_pay = ''\n",
    "                        break\n",
    "                    else:\n",
    "                        time.sleep(async_wait)\n",
    "                except:\n",
    "                    # error means page didn't load so try again\n",
    "                    time.sleep(async_wait)\n",
    "                    logging.error(\"Exception at 0005\")\n",
    "            except:\n",
    "                # error means page didn't load so try again\n",
    "                time.sleep(async_wait)\n",
    "                logging.error(\"Exception at 0006\")\n",
    "\n",
    "        # get company details and seperate on size and industry\n",
    "        company_size = '' # assigning as blanks as not important info and can skip if not obtained below\n",
    "        company_industry = ''\n",
    "        job_details = ''      \n",
    "        for attempt in range(retry_attempts):\n",
    "            try:\n",
    "                # TODO: since find_element_by_xpath is depreciated, let's replace with the right version\n",
    "                company_details = wd.find_element_by_xpath(\"//div[@class='mt5 mb2']/div[2]\").text\n",
    "                if \" · \" in company_details:\n",
    "                    company_size = company_details.split(\" · \")[0]\n",
    "                    company_industry = company_details.split(\" · \")[1]\n",
    "                else:\n",
    "                    company_size = company_details\n",
    "                    company_industry = ''\n",
    "                job_details = wd.find_element_by_id(\"job-details\").text.replace(\"\\n\", \" \")\n",
    "                break\n",
    "            except: \n",
    "                time.sleep(click_wait)\n",
    "                logging.error(\"Exception at 0007\")\n",
    "\n",
    "        # append (a) line to file\n",
    "        date_time = datetime.datetime.now().strftime(\"%d%b%Y-%H:%M:%S\")\n",
    "        search_keyword = search_keyword.replace(\"%20\", \" \")\n",
    "        list_job = [date_time, search_keyword, search_count, job_id, job_title, company, location, remote, update_time, applicants, job_pay, job_time, job_position, company_size, company_industry, job_details]\n",
    "        list_jobs.append(list_job)\n",
    "        logging.info(f\"Current job: {list_job}\")\n",
    "\n",
    "    with open(file, \"a\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(list_jobs)\n",
    "        print(list_jobs)\n",
    "        logging.info(\"Wrote a line to the csv\")\n",
    "        list_jobs = []\n",
    "    \n",
    "    logging.info(f\"Page {round(search_page/25) + 1} of {round(search_count/25)} loaded for {search_keyword}\")\n",
    "    search_page += 25\n",
    "\n",
    "    return search_page, search_count, url_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:77: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  wd = webdriver.Chrome(executable_path='./chromedriver', options=chrome_options)\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:79: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  wd.find_element_by_id('session_key').send_keys(LINKEDIN_USERNAME)\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:80: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  wd.find_element_by_id('session_password').send_keys(LINKEDIN_PASSWORD)\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:81: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  wd.find_element_by_xpath(\"//button[@class='sign-in-form__submit-button']\").click()\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:85: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  wd.find_element_by_xpath(\"//buton[@class='primary-action-new']\").click()\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:124: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  search_count = wd.find_element_by_css_selector(\"small.jobs-search-results-list__text\").text\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:132: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  search_results = wd.find_element_by_xpath(\"//ul[@class='jobs-search-results__list list-style-none']\").find_elements_by_tag_name(\"li\")\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:146: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  job = wd.find_element_by_id(res_id)\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:152: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  wd.find_element_by_xpath(f\"//div[@data-job-id='{job_id}']\").click()\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:164: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  job_title = wd.find_element_by_xpath(\"//h2[@class='t-24 t-bold']\")\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:181: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  job_top_card = wd.find_element_by_xpath(\"//span[@class='jobs-unified-top-card__subtitle-primary-grouping mr2 t-black']\").find_elements_by_tag_name(\"span\")\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:205: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  job_top_card2 = wd.find_element_by_xpath(\"//span[@class='jobs-unified-top-card__subtitle-secondary-grouping t-black--light']\").find_elements_by_tag_name(\"span\")\n",
      "/var/folders/39/nwfh4h5x7tx7_0zpz8y8kskc0000gn/T/ipykernel_3399/3767131222.py:264: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  company_details = wd.find_element_by_xpath(\"//div[@class='mt5 mb2']/div[2]\").text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['11Jan2022-12:13:14', 'data analyst', 3481, '2872944537', 'Sr. Data Analyst - Marketing Analytics (REMOTE)', 'Splunk', 'San Francisco, CA', 'Remote', '1 hour ago', '5', '', '', '', '', '', ''], ['11Jan2022-12:14:21', 'data analyst', 3481, '2872958846', 'Data Analyst', 'DISYS', 'Seattle, WA', 'Remote', '23 minutes ago', '3', '', '', '', '', '', ''], ['11Jan2022-12:15:27', 'data analyst', 3481, '2872929664', 'Senior Data Analyst, Marketing Analytics', 'Personal Capital', 'California, United States', 'Remote', '2 hours ago', '5', '', '', '', '', '', ''], ['11Jan2022-12:16:28', 'data analyst', 3481, '2874868176', 'Data Analyst', 'KTek Resourcing', 'Jersey City, NJ', 'Remote', '1 hour ago', '12', '', '', '', '', '', ''], ['11Jan2022-12:17:35', 'data analyst', 3481, '2871853861', 'Analytics Engineer', 'Pilot.com', 'United States', 'Remote', '18 hours ago', '15', '', '', '', '', '', ''], ['11Jan2022-12:18:41', 'data analyst', 3481, '2871867587', 'Data Engineer', 'Pilot.com', 'United States', 'Remote', '18 hours ago', '26', '', '', '', '', '', ''], ['11Jan2022-12:19:42', 'data analyst', 3481, '2874255139', 'Data Analyst - Wireless RF Engineering Project (Onsite or Remote)', 'Dice', 'Concord, CA', 'Remote', '19 hours ago', '8', '', '', '', '', '', ''], ['11Jan2022-12:20:49', 'data analyst', 3481, '2872589716', 'Data Quality Analyst', 'EveryMundo', 'Miami, FL', 'Remote', '4 hours ago', '17', '', '', '', '', '', ''], ['11Jan2022-12:21:55', 'data analyst', 3481, '2874377721', 'Data Analyst', 'Veho', 'United States', 'Remote', '13 hours ago', '65', '', '', '', '', '', ''], ['11Jan2022-12:22:56', 'data analyst', 3481, '2874716933', 'Data Analyst/Web Analytics - Minneapolis - Remote', 'DRS IT Solutions Inc', 'Minneapolis, MN', 'Remote', '7 hours ago', '27', '', '', '', '', '', ''], ['11Jan2022-12:23:47', 'data analyst', 3481, '2870729526', 'Sr. Analyst, Data Analyst- Remote', 'Inmar Intelligence', 'United States', 'Remote', '3 hours ago', '12', '', '', '', '', '', ''], ['11Jan2022-12:24:53', 'data analyst', 3481, '2871867499', 'Data Analytics Engineer', 'Pilot.com', 'United States', 'Remote', '18 hours ago', '24', '', '', '', '', '', ''], ['11Jan2022-12:25:55', 'data analyst', 3481, '2869673178', 'Analytics Engineer', 'Charter School Growth Fund', 'United States', 'Remote', '20 hours ago', '17', '', '', '', '', '', ''], ['11Jan2022-12:26:56', 'data analyst', 3481, '2874729856', 'Remote Data Analyst, Finance', 'Design Pickle', 'Scottsdale, AZ', 'Remote', '6 hours ago', '34', '', '', '', '', '', ''], ['11Jan2022-12:28:02', 'data analyst', 3481, '2870795034', 'Data Analyst Tester', 'Precision Technologies', 'United States', 'Remote', '34 minutes ago', '14', '', '', '', '', '', ''], ['11Jan2022-12:29:03', 'data analyst', 3481, '2872930902', 'Data Analyst', 'Personal Capital', 'United States', 'Remote', '2 hours ago', '32', '', '', '', '', '', ''], ['11Jan2022-12:30:10', 'data analyst', 3481, '2872572859', 'Analyst, BI and Data Delivery', 'Learning Care Group', 'United States', 'Remote', '5 hours ago', '44', '', '', '', '', '', ''], ['11Jan2022-12:31:16', 'data analyst', 3481, '2871886033', 'Marketing Data Analyst', 'Benchling', 'United States', 'Remote', '17 hours ago', '57', '', '', '', '', '', ''], ['11Jan2022-12:32:23', 'data analyst', 3481, '2872959351', 'Data Analyst', 'Veho', 'United States', 'Remote', '44 minutes ago', '13', '', '', '', '', '', ''], ['11Jan2022-12:33:24', 'data analyst', 3481, '2870710745', 'Data Analyst', 'Raas Infotek', 'United States', 'Remote', '1 hour ago', '11', '', '', '', '', '', ''], ['11Jan2022-12:34:25', 'data analyst', 3481, '2870709526', 'Program Data Analyst', 'New Leaders', 'United States', 'Remote', '4 hours ago', '35', '', '', '', '', '', ''], ['11Jan2022-12:35:31', 'data analyst', 3481, '2871454589', 'Data Analyst Looker Experience', 'Pilot.com', 'United States', 'Remote', '23 hours ago', '27', '', '', '', '', '', ''], ['11Jan2022-12:36:38', 'data analyst', 3481, '2874759013', 'Inside Sales Data Analyst', 'Applicantz', 'Denver, CO', 'Remote', '5 hours ago', '15', '', '', '', '', '', ''], ['11Jan2022-12:37:44', 'data analyst', 3481, '2873953770', 'MS SQL Data Analyst', 'StaffChase', 'Florida, United States', 'Remote', '23 hours ago', '56', '', '', '', '', '', ''], ['11Jan2022-12:38:45', 'data analyst', 3481, '2871829087', 'Senior Data Science Analyst (100% Remote US)', 'Dell Technologies', 'Hopkinton, MA', 'Remote', '19 hours ago', '7', '', '', '', '', '', '']]\n",
      "[['11Jan2022-12:40:19', 'data analyst', 3538, '2872910363', 'Senior Data Engineer – Systems Data Analyst (REMOTE)', 'GE Appliances, a Haier company', 'Minneapolis, MN', 'Remote', '4 hours ago', '3', '', '', '', '', '', ''], ['11Jan2022-12:41:26', 'data analyst', 3538, '2871816690', 'Data Analyst (Full-Time | Permanent)', 'The Sage Group (Bay Area)', 'United States', 'Remote', '19 hours ago', '173', '', '', '', '', '', ''], ['11Jan2022-12:42:27', 'data analyst', 3538, '2872959069', 'Data Analyst', 'ITECHSTACK INC', 'Chicago, IL', 'Remote', '1 hour ago', '6', '', '', '', '', '', ''], ['11Jan2022-12:43:28', 'data analyst', 3538, '2874886164', 'Data Analyst', 'PointClickCare', 'United States', 'Remote', '1 hour ago', '42', '', '', '', '', '', ''], ['11Jan2022-12:44:35', 'data analyst', 3538, '2870071482', 'Data Analyst / Senior Data Analyst (Remote Eligible - USA)', 'Dashlane', 'New York, NY', 'Remote', '5 hours ago', '25', '', '', '', '', '', ''], ['11Jan2022-12:45:41', 'data analyst', 3538, '2874766955', 'Sr. Data Analyst', 'The New Stack', 'United States', 'Remote', '4 hours ago', '19', '', '', '', '', '', ''], ['11Jan2022-12:46:42', 'data analyst', 3538, '2870067657', 'Data Analyst', 'display', 'Connecticut, United States', 'Remote', '6 hours ago', '68', '', '', '', '', '', ''], ['11Jan2022-12:47:49', 'data analyst', 3538, '2844483989', 'Data Analyst', 'Unum', 'Chattanooga, TN', 'Remote', '4 hours ago', '5', '', '', '', '', '', ''], ['11Jan2022-12:48:50', 'data analyst', 3538, '2872944537', 'Sr. Data Analyst - Marketing Analytics (REMOTE)', 'Splunk', 'San Francisco, CA', 'Remote', '2 hours ago', '5', '', '', '', '', '', ''], ['11Jan2022-12:49:41', 'data analyst', 3538, '2838411522', 'Sr. Data Analyst', 'Covetrus', 'Portland, ME', 'Remote', '6 hours ago', '3', '', '', '', '', '', ''], ['11Jan2022-12:50:32', 'data analyst', 3538, '2871829087', 'Senior Data Science Analyst (100% Remote US)', 'Dell Technologies', 'Hopkinton, MA', 'Remote', '19 hours ago', '7', '', '', '', '', '', ''], ['11Jan2022-12:51:38', 'data analyst', 3538, '2871458367', 'Data Analyst', 'Pilot.com', 'United States', 'Remote', '23 hours ago', '87', '', '', '', '', '', ''], ['11Jan2022-12:52:40', 'data analyst', 3538, '2871454589', 'Data Analyst Looker Experience', 'Pilot.com', 'United States', 'Remote', '23 hours ago', '27', '', '', '', '', '', ''], ['11Jan2022-12:53:41', 'data analyst', 3538, '2870729526', 'Sr. Analyst, Data Analyst- Remote', 'Inmar Intelligence', 'United States', 'Remote', '3 hours ago', '12', '', '', '', '', '', ''], ['11Jan2022-12:54:47', 'data analyst', 3538, '2874249794', 'Data Analyst', 'Dice', 'Woodcliff Lake, NJ', 'Remote', '19 hours ago', '16', '', '', '', '', '', ''], ['11Jan2022-12:55:54', 'data analyst', 3538, '2874760929', 'Data Analyst', 'Blue Chip Talent', 'Detroit Metropolitan Area', 'Remote', '5 hours ago', '36', '', '', '', '', '', ''], ['11Jan2022-12:57:00', 'data analyst', 3538, '2874883856', 'Big Data Analyst (US Remote)', 'Motorola Solutions', 'Schaumburg, IL', 'Remote', '1 hour ago', '9', '', '', '', '', '', ''], ['11Jan2022-12:58:06', 'data analyst', 3538, '2872184242', 'Clinical analyst/ data analyst', 'Swago', 'United States', 'Remote', '5 hours ago', '9', '', '', '', '', '', ''], ['11Jan2022-12:59:08', 'data analyst', 3538, '2871801466', 'Data Analyst, Product', 'Drip', 'United States', 'Remote', '21 hours ago', '133', '', '', '', '', '', ''], ['11Jan2022-13:00:09', 'data analyst', 3538, '2871889657', 'Clinical Analyst/Data Analyst', 'Swago', 'United States', 'Remote', '17 hours ago', '13', '', '', '', '', '', ''], ['11Jan2022-13:01:16', 'data analyst', 3538, '2872171095', 'Clinical Analyst/Data Analyst', 'Swago', 'United States', 'Remote', '10 hours ago', '22', '', '', '', '', '', ''], ['11Jan2022-13:02:22', 'data analyst', 3538, '2869652391', 'Data Analyst/Infrastructure Specialist', 'NYC Department of Health & Mental Hygiene Restaurant Inspection', 'New York City Metropolitan Area', 'Remote', '23 hours ago', '29', '', '', '', '', '', ''], ['11Jan2022-13:03:23', 'data analyst', 3538, '2872106645', 'Clinical Analyst /Data Analyst', 'Swago', 'United States', 'Remote', '16 hours ago', '19', '', '', '', '', '', ''], ['11Jan2022-13:04:14', 'data analyst', 3538, '2874770206', 'Clinical analyst/data analyst', 'Swago', 'United States', 'Remote', '5 hours ago', '20', '', '', '', '', '', ''], ['11Jan2022-13:05:16', 'data analyst', 3538, '2872556254', 'Clinical Analyst / Data Analyst', 'Swago', 'United States', 'Remote', '6 hours ago', '16', '', '', '', '', '', '']]\n"
     ]
    }
   ],
   "source": [
    "# create logging file\n",
    "logging = create_logfile()\n",
    "\n",
    "# create daily csv file\n",
    "date = datetime.date.today().strftime('%d-%b-%y')\n",
    "file = f\"data/{date}.csv\"\n",
    "create_file(file, logging)\n",
    "\n",
    "# login to linkedin and assign webdriver to variable\n",
    "wd = login(logging)\n",
    "\n",
    "# URL search terms here, try to limit to 3 or so\n",
    "# \n",
    "search_keywords = ['Data Analyst', 'Data Scientist', 'Data Engineer']\n",
    "search_location = \"United%20States\"\n",
    "search_remote = \"2\" # filter for remote positions\n",
    "search_posted = \"r86400\" # filter for past 24 hours\n",
    "\n",
    "# Counting Exceptions\n",
    "exception_first = 0\n",
    "exception_second = 0\n",
    "\n",
    "for search_keyword in search_keywords:\n",
    "    search_keyword = search_keyword.lower().replace(\" \", \"%20\")\n",
    "\n",
    "# Loop through each page and write results to csv\n",
    "    search_page = 0 # start on page 1\n",
    "    search_count = 1 # initiate search count until looks on page\n",
    "    while (search_page < search_count) and (search_page != 1000):\n",
    "        # Search each page and return location after each completion\n",
    "        try:\n",
    "            search_page, search_count, url_search = page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "        except Exception as e:\n",
    "            logging.error(f'(1) FIRST exception for {search_keyword} on {search_page} of {search_count}, retrying...')\n",
    "            logging.error(f'Current URL: {url_search}')\n",
    "            logging.error(e)\n",
    "            logging.exception('Traceback ->')\n",
    "            exception_first += 1\n",
    "            time.sleep(5) \n",
    "            try:\n",
    "                search_page, search_count, url_search = page_search(wd, search_location, search_keyword, search_remote, search_posted, search_page, search_count, file, logging)\n",
    "                logging.warning(f'Solved Exception for {search_keyword} on {search_page} of {search_count}')\n",
    "            except Exception as e:\n",
    "                logging.error(f'(2) SECOND exception remains for {search_keyword}. Skipping to next page...')\n",
    "                logging.error(f'Current URL: {url_search}')\n",
    "                logging.error(e)\n",
    "                logging.exception('Traceback ->')\n",
    "                search_page += 25 # skip to next page to avoid entry\n",
    "                exception_second += 1\n",
    "                logging.error(f'Skipping to next page for {search_keyword}, on {search_page} of {search_count}...')\n",
    "\n",
    "# close browser\n",
    "wd.quit()\n",
    "\n",
    "logging.info(f'LinkedIn data scraping complete with {exception_first} first and {exception_second} second exceptions')\n",
    "logging.info(f'Regard all further alarms...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa0fc083a9a7b25dab36cbe71fb89b2f1907d4eced1698b208dea6977346b521"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
